{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "[베이스라인]_데이콘 베이스라인 코드 (BERT) - 복사본.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W52zx-5npJ-e"
      },
      "source": [
        "# **1. 데이터 및 라이브러리 불러오기**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ih1qQvZspN3_"
      },
      "source": [
        "!pip install transformers==3.3.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Al9piRqSr20n"
      },
      "source": [
        "!pip install konlpy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ULeYWxKbpJ-j"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "import json\n",
        "import os\n",
        "import tqdm\n",
        "\n",
        "from konlpy.tag import Okt\n",
        "\n",
        "import sklearn\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "from sklearn.metrics import log_loss, accuracy_score,f1_score\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from transformers import *"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0CXw8z9KpJ-l"
      },
      "source": [
        "train=pd.read_csv('train.csv')\n",
        "test=pd.read_csv('test.csv')\n",
        "sample_submission=pd.read_csv('sample_submission.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2-TgPNK3pJ-l",
        "outputId": "42ac1ce9-1c43-4f1b-d7c6-950ed800956f"
      },
      "source": [
        "print(f'train.shape:{train.shape}')\n",
        "print(f'test.shape:{test.shape}')\n",
        "print(f'train label 개수: {train.label.nunique()}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train.shape:(174304, 13)\n",
            "test.shape:(43576, 12)\n",
            "train label 개수: 46\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0COiGdFVpJ-m"
      },
      "source": [
        "# **2. 데이터 전처리**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e9AVf1hBpJ-m"
      },
      "source": [
        "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_yscMJDspJ-m"
      },
      "source": [
        "#이번 베이스라인에서는 과제명 뿐만 아니라 요약문_연구내용도 모델에 학습시켜보겠습니다.\n",
        "train=train[['과제명', '요약문_연구내용','label']]\n",
        "test=test[['과제명', '요약문_연구내용']]\n",
        "train['요약문_연구내용'].fillna('NAN', inplace=True)\n",
        "test['요약문_연구내용'].fillna('NAN', inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23dw-Ti4pJ-n"
      },
      "source": [
        "train['data']=train['과제명']\n",
        "test['data']=test['과제명']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xGqspcKQpJ-n",
        "outputId": "09ed809b-ad05-4d2d-d9db-62f5cab37b45"
      },
      "source": [
        "print(train.shape)\n",
        "print(test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(174304, 4)\n",
            "(43576, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        },
        "id": "dKDuZSnGpJ-n",
        "outputId": "60c2c179-4e2f-4872-d90e-3b898961bccd"
      },
      "source": [
        "train.head(2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>과제명</th>\n",
              "      <th>요약문_연구내용</th>\n",
              "      <th>label</th>\n",
              "      <th>data</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>유전정보를 활용한 새로운 해충 분류군 동정기술 개발</td>\n",
              "      <td>(가) 외래 및 돌발해충의 발생조사 및 종 동정\\n\\n\\n    ○ 대상해충 : 최...</td>\n",
              "      <td>24</td>\n",
              "      <td>유전정보를 활용한 새로운 해충 분류군 동정기술 개발</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>대장암의 TRAIL 내성 표적 인자 발굴 및 TRAIL 반응 예측 유전자 지도 구축...</td>\n",
              "      <td>1차년도\\n1) Microarray를 통한 선천적 TRAIL 내성 표적 후보 유전자...</td>\n",
              "      <td>0</td>\n",
              "      <td>대장암의 TRAIL 내성 표적 인자 발굴 및 TRAIL 반응 예측 유전자 지도 구축...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                 과제명  ...                                               data\n",
              "0                       유전정보를 활용한 새로운 해충 분류군 동정기술 개발  ...                       유전정보를 활용한 새로운 해충 분류군 동정기술 개발\n",
              "1  대장암의 TRAIL 내성 표적 인자 발굴 및 TRAIL 반응 예측 유전자 지도 구축...  ...  대장암의 TRAIL 내성 표적 인자 발굴 및 TRAIL 반응 예측 유전자 지도 구축...\n",
              "\n",
              "[2 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        },
        "id": "t4R-43eqpJ-o",
        "outputId": "35f66055-5c7f-43cd-aafb-149fb4753f5b"
      },
      "source": [
        "test.head(2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>과제명</th>\n",
              "      <th>요약문_연구내용</th>\n",
              "      <th>data</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>R-FSSW 기술 적용 경량 차체 부품 개발 및 품질 평가를 위한 64채널 C-SC...</td>\n",
              "      <td>○ 1차년도\\n\\n    . 개발 탐촉 시스템의 성능 평가 위한 표준 시편 제작 시...</td>\n",
              "      <td>R-FSSW 기술 적용 경량 차체 부품 개발 및 품질 평가를 위한 64채널 C-SC...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>다입자계를 묘사하는 편미분방정식에 대한 연구</td>\n",
              "      <td>연구과제1. 무한입자계의 동역학 / 작용소(operator) 방정식에 대한 연구\\n...</td>\n",
              "      <td>다입자계를 묘사하는 편미분방정식에 대한 연구</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                 과제명  ...                                               data\n",
              "0  R-FSSW 기술 적용 경량 차체 부품 개발 및 품질 평가를 위한 64채널 C-SC...  ...  R-FSSW 기술 적용 경량 차체 부품 개발 및 품질 평가를 위한 64채널 C-SC...\n",
              "1                           다입자계를 묘사하는 편미분방정식에 대한 연구  ...                           다입자계를 묘사하는 편미분방정식에 대한 연구\n",
              "\n",
              "[2 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k4Gk0irkpJ-o"
      },
      "source": [
        "# **3. 모델링**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jE2xtSgdpJ-o"
      },
      "source": [
        "#random seed 고정\n",
        "tf.random.set_seed(1234)\n",
        "np.random.seed(1234)\n",
        "BATCH_SIZE = 32\n",
        "NUM_EPOCHS = 3\n",
        "VALID_SPLIT = 0.2\n",
        "MAX_LEN=200"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bko4xAFcpJ-p",
        "outputId": "5a9946ce-e4ce-4cbd-f1cd-81ead750c0cf"
      },
      "source": [
        "from transformers import *\n",
        "tokenizer=BertTokenizer.from_pretrained('bert-base-multilingual-cased',  cache_dir='bert_ckpt', do_lower_case=False)\n",
        "\n",
        "def bert_tokenizer(sent, MAX_LEN):\n",
        "    \n",
        "    encoded_dict=tokenizer.encode_plus(\n",
        "    text = sent, \n",
        "    add_special_tokens=True, \n",
        "    max_length=MAX_LEN, \n",
        "    pad_to_max_length=True, \n",
        "    return_attention_mask=True,\n",
        "    truncation = True)\n",
        "    \n",
        "    input_id=encoded_dict['input_ids']\n",
        "    attention_mask=encoded_dict['attention_mask']\n",
        "    token_type_id = encoded_dict['token_type_ids']\n",
        "    \n",
        "    return input_id, attention_mask, token_type_id\n",
        "\n",
        "input_ids =[]\n",
        "attention_masks =[]\n",
        "token_type_ids =[]\n",
        "train_data_labels = []\n",
        "\n",
        "def clean_text(sent):\n",
        "    sent_clean=re.sub(\"[^가-힣ㄱ-하-ㅣ]\", \" \", sent)\n",
        "    return sent_clean\n",
        "\n",
        "for train_sent, train_label in zip(train['data'], train['label']):\n",
        "    try:\n",
        "        input_id, attention_mask, token_type_id = bert_tokenizer(clean_text(train_sent), MAX_LEN=MAX_LEN)\n",
        "        \n",
        "        input_ids.append(input_id)\n",
        "        attention_masks.append(attention_mask)\n",
        "        token_type_ids.append(token_type_id)\n",
        "        #########################################\n",
        "        train_data_labels.append(train_label)\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "        print(train_sent)\n",
        "        pass\n",
        "\n",
        "train_input_ids=np.array(input_ids, dtype=int)\n",
        "train_attention_masks=np.array(attention_masks, dtype=int)\n",
        "train_token_type_ids=np.array(token_type_ids, dtype=int)\n",
        "###########################################################\n",
        "train_inputs=(train_input_ids, train_attention_masks, train_token_type_ids)\n",
        "train_labels=np.asarray(train_data_labels, dtype=np.int32)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:1773: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MJjCsSAYpJ-p",
        "outputId": "9204d665-bbda-4fe4-9284-0025ede4cefa"
      },
      "source": [
        "print(train_input_ids[1])\n",
        "print(train_attention_masks[1])\n",
        "print(train_token_type_ids[1])\n",
        "print(tokenizer.decode(train_input_ids[1]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[   101   9069  13890 119115  10459   8996  17138   9934  14801   9640\n",
            "  13764   9323 118654   9316   9321 119187   9576 119281   9625  16617\n",
            "  13764   9706  12092   8908  70122  10530  42300  91785    102      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0\n",
            "      0      0      0      0      0      0      0      0      0      0]\n",
            "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "[CLS] 대장암의 내성 표적 인자 발굴 및 반응 예측 유전자 지도 구축에 관한 연구 [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-3Ct4SyHKpW1"
      },
      "source": [
        "from keras import backend as K\n",
        "\n",
        "def f1(y_true, y_pred):\n",
        "    def recall(y_true, y_pred):\n",
        "        \"\"\"Recall metric.\n",
        "\n",
        "        Only computes a batch-wise average of recall.\n",
        "\n",
        "        Computes the recall, a metric for multi-label classification of\n",
        "        how many relevant items are selected.\n",
        "        \"\"\"\n",
        "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "        recall = true_positives / (possible_positives + K.epsilon())\n",
        "        return recall\n",
        "\n",
        "    def precision(y_true, y_pred):\n",
        "        \"\"\"Precision metric.\n",
        "\n",
        "        Only computes a batch-wise average of precision.\n",
        "\n",
        "        Computes the precision, a metric for multi-label classification of\n",
        "        how many selected items are relevant.\n",
        "        \"\"\"\n",
        "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "        precision = true_positives / (predicted_positives + K.epsilon())\n",
        "        return precision\n",
        "    precision = precision(y_true, y_pred)\n",
        "    recall = recall(y_true, y_pred)\n",
        "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vCEbIyxFpJ-q"
      },
      "source": [
        "class TFBertClassifier(tf.keras.Model):\n",
        "    def __init__(self, model_name, dir_path, num_class):\n",
        "        super(TFBertClassifier, self).__init__()\n",
        "\n",
        "        self.bert = TFBertModel.from_pretrained(model_name, cache_dir=dir_path)\n",
        "        self.dropout = tf.keras.layers.Dropout(self.bert.config.hidden_dropout_prob)\n",
        "        self.classifier = tf.keras.layers.Dense(num_class, \n",
        "                                                kernel_initializer=tf.keras.initializers.TruncatedNormal(self.bert.config.initializer_range), \n",
        "                                                name=\"classifier\")\n",
        "        \n",
        "    def call(self, inputs, attention_mask=None, token_type_ids=None, training=False):\n",
        "        \n",
        "        #outputs 값: # sequence_output, pooled_output, (hidden_states), (attentions)\n",
        "        outputs = self.bert(inputs, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
        "        pooled_output = outputs[1] \n",
        "        pooled_output = self.dropout(pooled_output, training=training)\n",
        "        logits = self.classifier(pooled_output)\n",
        "\n",
        "        return logits\n",
        "\n",
        "cls_model = TFBertClassifier(model_name='bert-base-multilingual-cased',\n",
        "                                  dir_path='bert_ckpt',\n",
        "                                  num_class=46)\n",
        "\n",
        "# 학습 준비하기\n",
        "optimizer = tf.keras.optimizers.Adam(3e-5)\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
        "#metric = f1\n",
        "cls_model.compile(optimizer=optimizer, loss=loss, metrics=[metric])\n",
        "\n",
        "model_name = \"tf2_bert_classifier\"\n",
        "\n",
        "# overfitting을 막기 위한 ealrystop 추가\n",
        "earlystop_callback = EarlyStopping(monitor='val_accuracy', min_delta=0.0001,patience=5)\n",
        "# min_delta: the threshold that triggers the termination (acc should at least improve 0.0001)\n",
        "# patience: no improvment epochs (patience = 1, 1번 이상 상승이 없으면 종료)\\\n",
        "\n",
        "checkpoint_path = os.path.join(model_name, 'weights.h5')\n",
        "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
        "\n",
        "# Create path if exists\n",
        "if os.path.exists(checkpoint_dir):\n",
        "    print(\"{} -- Folder already exists \\n\".format(checkpoint_dir))\n",
        "else:\n",
        "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "    print(\"{} -- Folder create complete \\n\".format(checkpoint_dir))\n",
        "    \n",
        "cp_callback = ModelCheckpoint(\n",
        "    checkpoint_path, monitor='val_accuracy', verbose=1, save_best_only=True, save_weights_only=True)\n",
        "\n",
        "# 학습과 eval 시작\n",
        "history = cls_model.fit(train_inputs, train_labels, epochs=30, batch_size=64,\n",
        "                    validation_split = VALID_SPLIT, callbacks=[earlystop_callback, cp_callback])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jkUO91WUpJ-r"
      },
      "source": [
        "input_ids =[]\n",
        "attention_masks =[]\n",
        "token_type_ids =[]\n",
        "train_data_labels = []\n",
        "\n",
        "def clean_text(sent):\n",
        "    sent_clean=re.sub(\"[^가-힣ㄱ-하-ㅣ]\", \" \", sent)\n",
        "    return sent_clean\n",
        "\n",
        "for test_sent in test['data']:\n",
        "    try:\n",
        "        input_id, attention_mask, token_type_id = bert_tokenizer(clean_text(test_sent), MAX_LEN=40)\n",
        "        \n",
        "        input_ids.append(input_id)\n",
        "        attention_masks.append(attention_mask)\n",
        "        token_type_ids.append(token_type_id)\n",
        "        #########################################\n",
        "       \n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "        print(test_sent)\n",
        "        pass\n",
        "    \n",
        "test_input_ids=np.array(input_ids, dtype=int)\n",
        "test_attention_masks=np.array(attention_masks, dtype=int)\n",
        "test_token_type_ids=np.array(token_type_ids, dtype=int)\n",
        "###########################################################\n",
        "test_inputs=(test_input_ids, test_attention_masks, test_token_type_ids)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R3vPLe6dpJ-s"
      },
      "source": [
        "results = cls_model.predict(test_inputs)\n",
        "results=tf.argmax(results, axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cn_0-w8FpJ-s"
      },
      "source": [
        "sample_submission['label']=results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wRn4fBnPpJ-s"
      },
      "source": [
        "sample_submission"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lcXmFrXKpJ-s"
      },
      "source": [
        "sample_submission.to_csv('bert_baseline.csv', index=False)   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tad2p2f5qXks"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}