{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Dacon.ipynb",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qOetC_RyxBGC",
        "outputId": "00d63d86-c910-4e82-eab8-ffb8c4619c48"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ym1AMIoRJS9J",
        "outputId": "3b5f9214-2844-4a94-d54c-86d1d82447ea"
      },
      "source": [
        "cd /content/drive/My Drive/mulcamnlp2021/Mecab-ko-for-Google-Colab/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/mulcamnlp2021/Mecab-ko-for-Google-Colab\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dPYyLyc1LAE7",
        "outputId": "426b318b-de2b-4203-c365-d205cd63adc7"
      },
      "source": [
        "! bash install_mecab-ko_on_colab190912.sh"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Installing konlpy.....\n",
            "Collecting konlpy\n",
            "  Downloading konlpy-0.5.2-py2.py3-none-any.whl (19.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 19.4 MB 229 kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.19.5)\n",
            "Requirement already satisfied: tweepy>=3.7.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (3.10.0)\n",
            "Collecting beautifulsoup4==4.6.0\n",
            "  Downloading beautifulsoup4-4.6.0-py3-none-any.whl (86 kB)\n",
            "\u001b[K     |████████████████████████████████| 86 kB 7.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (4.2.6)\n",
            "Collecting JPype1>=0.7.0\n",
            "  Downloading JPype1-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (448 kB)\n",
            "\u001b[K     |████████████████████████████████| 448 kB 100.0 MB/s \n",
            "\u001b[?25hCollecting colorama\n",
            "  Downloading colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from JPype1>=0.7.0->konlpy) (3.7.4.3)\n",
            "Requirement already satisfied: requests[socks]>=2.11.1 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (2.23.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (1.3.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (1.15.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.1.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2021.5.30)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.7.1)\n",
            "Installing collected packages: JPype1, colorama, beautifulsoup4, konlpy\n",
            "  Attempting uninstall: beautifulsoup4\n",
            "    Found existing installation: beautifulsoup4 4.6.3\n",
            "    Uninstalling beautifulsoup4-4.6.3:\n",
            "      Successfully uninstalled beautifulsoup4-4.6.3\n",
            "Successfully installed JPype1-1.3.0 beautifulsoup4-4.6.0 colorama-0.4.4 konlpy-0.5.2\n",
            "Done\n",
            "Installing mecab-0.996-ko-0.9.2.tar.gz.....\n",
            "Downloading mecab-0.996-ko-0.9.2.tar.gz.......\n",
            "from https://bitbucket.org/eunjeon/mecab-ko/downloads/mecab-0.996-ko-0.9.2.tar.gz\n",
            "--2021-08-03 07:50:14--  https://bitbucket.org/eunjeon/mecab-ko/downloads/mecab-0.996-ko-0.9.2.tar.gz\n",
            "Resolving bitbucket.org (bitbucket.org)... 104.192.141.1, 2406:da00:ff00::22c3:9b0a, 2406:da00:ff00::34cc:ea4a, ...\n",
            "Connecting to bitbucket.org (bitbucket.org)|104.192.141.1|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://bbuseruploads.s3.amazonaws.com/eunjeon/mecab-ko/downloads/mecab-0.996-ko-0.9.2.tar.gz?Signature=BCKuNotjHvpDGX1Xnh5eLsjgo4E%3D&Expires=1627978814&AWSAccessKeyId=AKIA6KOSE3BNJRRFUUX6&versionId=null&response-content-disposition=attachment%3B%20filename%3D%22mecab-0.996-ko-0.9.2.tar.gz%22&response-content-encoding=None [following]\n",
            "--2021-08-03 07:50:14--  https://bbuseruploads.s3.amazonaws.com/eunjeon/mecab-ko/downloads/mecab-0.996-ko-0.9.2.tar.gz?Signature=BCKuNotjHvpDGX1Xnh5eLsjgo4E%3D&Expires=1627978814&AWSAccessKeyId=AKIA6KOSE3BNJRRFUUX6&versionId=null&response-content-disposition=attachment%3B%20filename%3D%22mecab-0.996-ko-0.9.2.tar.gz%22&response-content-encoding=None\n",
            "Resolving bbuseruploads.s3.amazonaws.com (bbuseruploads.s3.amazonaws.com)... 52.217.171.249\n",
            "Connecting to bbuseruploads.s3.amazonaws.com (bbuseruploads.s3.amazonaws.com)|52.217.171.249|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1414979 (1.3M) [application/x-tar]\n",
            "Saving to: ‘mecab-0.996-ko-0.9.2.tar.gz’\n",
            "\n",
            "mecab-0.996-ko-0.9. 100%[===================>]   1.35M  1.21MB/s    in 1.1s    \n",
            "\n",
            "2021-08-03 07:50:16 (1.21 MB/s) - ‘mecab-0.996-ko-0.9.2.tar.gz’ saved [1414979/1414979]\n",
            "\n",
            "Done\n",
            "Unpacking mecab-0.996-ko-0.9.2.tar.gz.......\n",
            "Done\n",
            "Change Directory to mecab-0.996-ko-0.9.2.......\n",
            "installing mecab-0.996-ko-0.9.2.tar.gz........\n",
            "configure\n",
            "make\n",
            "make check\n",
            "make install\n",
            "ldconfig\n",
            "Done\n",
            "Change Directory to /content\n",
            "Downloading mecab-ko-dic-2.1.1-20180720.tar.gz.......\n",
            "from https://bitbucket.org/eunjeon/mecab-ko-dic/downloads/mecab-ko-dic-2.1.1-20180720.tar.gz\n",
            "--2021-08-03 07:51:30--  https://bitbucket.org/eunjeon/mecab-ko-dic/downloads/mecab-ko-dic-2.1.1-20180720.tar.gz\n",
            "Resolving bitbucket.org (bitbucket.org)... 104.192.141.1, 2406:da00:ff00::34cc:ea4a, 2406:da00:ff00::22c2:513, ...\n",
            "Connecting to bitbucket.org (bitbucket.org)|104.192.141.1|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://bbuseruploads.s3.amazonaws.com/a4fcd83e-34f1-454e-a6ac-c242c7d434d3/downloads/b5a0c703-7b64-45ed-a2d7-180e962710b6/mecab-ko-dic-2.1.1-20180720.tar.gz?Signature=Q81%2BQhCkmb7CjX9nTqohxcZBnDE%3D&Expires=1627978728&AWSAccessKeyId=AKIA6KOSE3BNJRRFUUX6&versionId=tzyxc1TtnZU_zEuaaQDGN4F76hPDpyFq&response-content-disposition=attachment%3B%20filename%3D%22mecab-ko-dic-2.1.1-20180720.tar.gz%22&response-content-encoding=None [following]\n",
            "--2021-08-03 07:51:30--  https://bbuseruploads.s3.amazonaws.com/a4fcd83e-34f1-454e-a6ac-c242c7d434d3/downloads/b5a0c703-7b64-45ed-a2d7-180e962710b6/mecab-ko-dic-2.1.1-20180720.tar.gz?Signature=Q81%2BQhCkmb7CjX9nTqohxcZBnDE%3D&Expires=1627978728&AWSAccessKeyId=AKIA6KOSE3BNJRRFUUX6&versionId=tzyxc1TtnZU_zEuaaQDGN4F76hPDpyFq&response-content-disposition=attachment%3B%20filename%3D%22mecab-ko-dic-2.1.1-20180720.tar.gz%22&response-content-encoding=None\n",
            "Resolving bbuseruploads.s3.amazonaws.com (bbuseruploads.s3.amazonaws.com)... 52.217.88.132\n",
            "Connecting to bbuseruploads.s3.amazonaws.com (bbuseruploads.s3.amazonaws.com)|52.217.88.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 49775061 (47M) [application/x-tar]\n",
            "Saving to: ‘mecab-ko-dic-2.1.1-20180720.tar.gz’\n",
            "\n",
            "mecab-ko-dic-2.1.1- 100%[===================>]  47.47M  12.8MB/s    in 4.5s    \n",
            "\n",
            "2021-08-03 07:51:36 (10.7 MB/s) - ‘mecab-ko-dic-2.1.1-20180720.tar.gz’ saved [49775061/49775061]\n",
            "\n",
            "Done\n",
            "Unpacking  mecab-ko-dic-2.1.1-20180720.tar.gz.......\n",
            "Done\n",
            "Change Directory to mecab-ko-dic-2.1.1-20180720\n",
            "Done\n",
            "installing........\n",
            "configure\n",
            "make\n",
            "make install\n",
            "apt-get update\n",
            "apt-get upgrade\n",
            "apt install curl\n",
            "apt install git\n",
            "bash <(curl -s https://raw.githubusercontent.com/konlpy/konlpy/master/scripts/mecab.sh)\n",
            "Done\n",
            "Successfully Installed\n",
            "Now you can use Mecab\n",
            "from konlpy.tag import Mecab\n",
            "mecab = Mecab()\n",
            "사용자 사전 추가 방법 : https://bit.ly/3k0ZH53\n",
            "NameError: name 'Tagger' is not defined 오류 발생 시 런타임을 재실행 해주세요\n",
            "블로그에 해결 방법을 남겨주신 tana님 감사합니다.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CcdZSsNDM19E",
        "outputId": "3afb4f53-9e47-4a32-c701-8fd133c68d2a"
      },
      "source": [
        "!pip install wandb"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting wandb\n",
            "  Downloading wandb-0.11.2-py2.py3-none-any.whl (1.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.8 MB 4.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
            "Collecting docker-pycreds>=0.4.0\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Collecting configparser>=3.8.1\n",
            "  Downloading configparser-5.0.2-py3-none-any.whl (19 kB)\n",
            "Collecting pathtools\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
            "Collecting GitPython>=1.0.0\n",
            "  Downloading GitPython-3.1.18-py3-none-any.whl (170 kB)\n",
            "\u001b[K     |████████████████████████████████| 170 kB 18.1 MB/s \n",
            "\u001b[?25hCollecting subprocess32>=3.5.3\n",
            "  Downloading subprocess32-3.5.4.tar.gz (97 kB)\n",
            "\u001b[K     |████████████████████████████████| 97 kB 6.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (3.13)\n",
            "Collecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.3.1-py2.py3-none-any.whl (133 kB)\n",
            "\u001b[K     |████████████████████████████████| 133 kB 18.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
            "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.17.3)\n",
            "Collecting urllib3>=1.26.5\n",
            "  Downloading urllib3-1.26.6-py2.py3-none-any.whl (138 kB)\n",
            "\u001b[K     |████████████████████████████████| 138 kB 21.2 MB/s \n",
            "\u001b[?25hCollecting shortuuid>=0.5.0\n",
            "  Downloading shortuuid-1.0.1-py3-none-any.whl (7.5 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.8.1)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.7-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 2.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.0 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (3.7.4.3)\n",
            "Collecting smmap<5,>=3.0.1\n",
            "  Downloading smmap-4.0.0-py2.py3-none-any.whl (24 kB)\n",
            "Collecting requests<3,>=2.0.0\n",
            "  Downloading requests-2.26.0-py2.py3-none-any.whl (62 kB)\n",
            "\u001b[K     |████████████████████████████████| 62 kB 1.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2021.5.30)\n",
            "Building wheels for collected packages: subprocess32, pathtools\n",
            "  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for subprocess32: filename=subprocess32-3.5.4-py3-none-any.whl size=6502 sha256=a02c650b8c3ad90823aa928f18568c12552964a6150f93568ad2c9e2ff490347\n",
            "  Stored in directory: /root/.cache/pip/wheels/50/ca/fa/8fca8d246e64f19488d07567547ddec8eb084e8c0d7a59226a\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8806 sha256=7c3746412314880c7945ef7269d6e110a7708582f1a4ab68a6222c56893f80c1\n",
            "  Stored in directory: /root/.cache/pip/wheels/3e/31/09/fa59cef12cdcfecc627b3d24273699f390e71828921b2cbba2\n",
            "Successfully built subprocess32 pathtools\n",
            "Installing collected packages: smmap, urllib3, gitdb, subprocess32, shortuuid, sentry-sdk, requests, pathtools, GitPython, docker-pycreds, configparser, wandb\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests~=2.23.0, but you have requests 2.26.0 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed GitPython-3.1.18 configparser-5.0.2 docker-pycreds-0.4.0 gitdb-4.0.7 pathtools-0.1.2 requests-2.26.0 sentry-sdk-1.3.1 shortuuid-1.0.1 smmap-4.0.0 subprocess32-3.5.4 urllib3-1.26.6 wandb-0.11.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "818BAzAyLs9k",
        "outputId": "be045b26-563a-4f07-878f-64812895fd92"
      },
      "source": [
        "import wandb\n",
        "from wandb.keras import WandbCallback\n",
        "\n",
        "wandb.login()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 618
        },
        "id": "KwXXuA58UzdU",
        "outputId": "bdcd051d-a43f-4ffc-b971-a7115396c077"
      },
      "source": [
        "wandb.init(project=\"dacon_climate_project\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdasom-oh\u001b[0m (use `wandb login --relogin` to force relogin)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                Tracking run with wandb version 0.11.2<br/>\n",
              "                Syncing run <strong style=\"color:#cdcd00\">drawn-wood-5</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://wandb.ai/dasom-oh/dacon_climate_project\" target=\"_blank\">https://wandb.ai/dasom-oh/dacon_climate_project</a><br/>\n",
              "                Run page: <a href=\"https://wandb.ai/dasom-oh/dacon_climate_project/runs/6yaztes8\" target=\"_blank\">https://wandb.ai/dasom-oh/dacon_climate_project/runs/6yaztes8</a><br/>\n",
              "                Run data is saved locally in <code>/content/drive/My Drive/mulcamnlp2021/Mecab-ko-for-Google-Colab/wandb/run-20210803_075623-6yaztes8</code><br/><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7f8a170f44d0>"
            ],
            "text/html": [
              "<h1>Run(6yaztes8)</h1><iframe src=\"https://wandb.ai/dasom-oh/dacon_climate_project/runs/6yaztes8\" style=\"border:none;width:100%;height:400px\"></iframe>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ubvOfODNN7f2",
        "outputId": "5805ec09-05b5-4729-aaac-2871ff810e3f"
      },
      "source": [
        "pip install tensorflow-addons"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow-addons\n",
            "  Downloading tensorflow_addons-0.13.0-cp37-cp37m-manylinux2010_x86_64.whl (679 kB)\n",
            "\u001b[?25l\r\u001b[K     |▌                               | 10 kB 36.2 MB/s eta 0:00:01\r\u001b[K     |█                               | 20 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |█▌                              | 30 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |██                              | 40 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |██▍                             | 51 kB 3.9 MB/s eta 0:00:01\r\u001b[K     |███                             | 61 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |███▍                            | 71 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |███▉                            | 81 kB 4.8 MB/s eta 0:00:01\r\u001b[K     |████▍                           | 92 kB 4.8 MB/s eta 0:00:01\r\u001b[K     |████▉                           | 102 kB 4.0 MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 112 kB 4.0 MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 122 kB 4.0 MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 133 kB 4.0 MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 143 kB 4.0 MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 153 kB 4.0 MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 163 kB 4.0 MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 174 kB 4.0 MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 184 kB 4.0 MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 194 kB 4.0 MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 204 kB 4.0 MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 215 kB 4.0 MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 225 kB 4.0 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 235 kB 4.0 MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 245 kB 4.0 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 256 kB 4.0 MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 266 kB 4.0 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 276 kB 4.0 MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 286 kB 4.0 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 296 kB 4.0 MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 307 kB 4.0 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 317 kB 4.0 MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 327 kB 4.0 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 337 kB 4.0 MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 348 kB 4.0 MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 358 kB 4.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 368 kB 4.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 378 kB 4.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 389 kB 4.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 399 kB 4.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 409 kB 4.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 419 kB 4.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 430 kB 4.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 440 kB 4.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 450 kB 4.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 460 kB 4.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 471 kB 4.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 481 kB 4.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 491 kB 4.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 501 kB 4.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 512 kB 4.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 522 kB 4.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 532 kB 4.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 542 kB 4.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 552 kB 4.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 563 kB 4.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 573 kB 4.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 583 kB 4.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 593 kB 4.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 604 kB 4.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 614 kB 4.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 624 kB 4.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 634 kB 4.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 645 kB 4.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 655 kB 4.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 665 kB 4.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 675 kB 4.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 679 kB 4.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons) (2.7.1)\n",
            "Installing collected packages: tensorflow-addons\n",
            "Successfully installed tensorflow-addons-0.13.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bqqItxYOOM6e",
        "outputId": "fe4fdaf5-3150-4428-f7ee-e315fae59ec7"
      },
      "source": [
        "!pip install transformers==3.3.0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers==3.3.0\n",
            "  Downloading transformers-3.3.0-py3-none-any.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 4.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from transformers==3.3.0) (1.19.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==3.3.0) (4.41.1)\n",
            "Collecting sentencepiece!=0.1.92\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 15.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==3.3.0) (2019.12.20)\n",
            "Collecting tokenizers==0.8.1.rc2\n",
            "  Downloading tokenizers-0.8.1rc2-cp37-cp37m-manylinux1_x86_64.whl (3.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0 MB 23.8 MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "  Downloading sacremoses-0.0.45-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 39.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==3.3.0) (2.26.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==3.3.0) (21.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==3.3.0) (3.0.12)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==3.3.0) (2.4.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.3.0) (2021.5.30)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.3.0) (2.0.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.3.0) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.3.0) (1.26.6)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.3.0) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.3.0) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.3.0) (1.15.0)\n",
            "Installing collected packages: tokenizers, sentencepiece, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.45 sentencepiece-0.1.96 tokenizers-0.8.1rc2 transformers-3.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rppUXJxtOp5T",
        "outputId": "f7c29906-e322-46cb-de95-f88e68255f03"
      },
      "source": [
        " pip install sentencepiece"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (0.1.96)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "53i4a_6dLC5F"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "import json\n",
        "import os\n",
        "from tensorflow.python.keras.layers.core import Dropout\n",
        "# from tensorflow.python.keras.backend import dtype, learning_phase\n",
        "import tqdm\n",
        "import tensorflow_addons as tfa\n",
        "from konlpy.tag import Mecab\n",
        "\n",
        "from sklearn.metrics import accuracy_score,f1_score,precision_score, recall_score\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from transformers import *\n",
        "from tensorflow.keras import Model, layers\n",
        "from tensorflow.keras.layers import Input, MaxPool1D, GlobalAveragePooling1D, Dense, Embedding, Lambda, BatchNormalization\n",
        "from tensorflow.keras.utils import Sequence\n",
        "import math"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iG1KYclRNfDr"
      },
      "source": [
        "tf.random.set_seed(1234)\n",
        "np.random.seed(1234)\n",
        "BATCH_SIZE = 32\n",
        "NUM_EPOCHS = 3\n",
        "VALID_SPLIT = 0.2\n",
        "MAX_LEN=200\n",
        "DATA_IN_PATH='/content/drive/My Drive/mulcamnlp2021/datain/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "trIygUHjQwHq"
      },
      "source": [
        "try:\n",
        "    with open(DATA_IN_PATH+'train_input.npy', 'rb') as f1, open(DATA_IN_PATH+'label_input.npy', 'rb') as f2, open(DATA_IN_PATH+'test_input.npy', 'rb') as f3, open(DATA_IN_PATH+'data_configs.json', 'rb') as f4:\n",
        "        inputs_on_train= np.load(f1, allow_pickle=True)\n",
        "        labels= np.load(f2, allow_pickle=True)\n",
        "        test_inputs= np.load(f3, allow_pickle=True)\n",
        "        data_configs = json.load(f4)\n",
        "except:\n",
        "\n",
        "    try:\n",
        "        with open(DATA_IN_PATH+'clean_train_text.npy', 'rb') as f1, open(DATA_IN_PATH+'clean_test_text.npy', 'rb') as f2, open(DATA_IN_PATH+'label_input.npy', 'rb') as f3:\n",
        "            clean_train_text= np.load(f1, allow_pickle=True)\n",
        "            clean_test_text= np.load(f2, allow_pickle=True)\n",
        "            labels= np.load(f3, allow_pickle=True)\n",
        "    except:\n",
        "        train=pd.read_csv(DATA_IN_PATH+'train.csv')\n",
        "        test=pd.read_csv(DATA_IN_PATH+'test.csv')\n",
        "\n",
        "        temp_col='data'\n",
        "\n",
        "        for col in train.columns:\n",
        "            try:\n",
        "                train[temp_col] = train[temp_col] + train[col].astype(str)\n",
        "                test[temp_col] = test[temp_col] + test[col].astype(str)\n",
        "            except KeyError:\n",
        "                if col !='label':\n",
        "                    train[temp_col] = train[col].astype(str)\n",
        "                    test[temp_col] = test[col].astype(str)\n",
        "\n",
        "        train[temp_col].fillna('NAN', inplace=True)\n",
        "        test[temp_col].fillna('NAN', inplace=True)\n",
        "\n",
        "        labels=train['label'].iloc[:100]\n",
        "\n",
        "        train=train[[temp_col, 'label']].iloc[:100]\n",
        "        test=test[[temp_col]].iloc[:100]\n",
        "\n",
        "        from nltk.tokenize import word_tokenize\n",
        "        from nltk.corpus import stopwords\n",
        "        from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "        def preprocessing(text, tokenizer, remove_stopwords=False, stop_words_kr=[], stop_words_en=[]):\n",
        "            kr=re.sub(\"[a-zA-Z\\s]\",\"\", text)\n",
        "            word_text=tokenizer.pos(kr)\n",
        "            if remove_stopwords:\n",
        "                word_text=[token[0] for token in word_text if not token[0] in stop_words_kr and token[1] not in ['SSO', 'SY', 'SC', 'SSC', 'SN']]\n",
        "\n",
        "            eng=re.sub(\"[가-힣ㄱ-ㅎㅏ-ㅣ]\",\"\", text)\n",
        "            word_tokens = word_tokenize(eng, language='english')\n",
        "\n",
        "            wl = WordNetLemmatizer()\n",
        "            for word in word_tokens: \n",
        "                word = word.strip().lower()       \n",
        "                if word not in stop_words_en:\n",
        "                    if len(word) > 2: # 단어 길이가 2이하인 경우에 대하여 추가로 단어를 제거합니다.\n",
        "                        word_text.append(wl.lemmatize(word))\n",
        "            return word_text\n",
        "\n",
        "        stop_words_en = set(stopwords.words('english'))\n",
        "        clean_train_text=[]\n",
        "        clean_test_text=[]\n",
        "\n",
        "        mecab = Mecab()#210625 되는지 확인해보기\n",
        "\n",
        "        import sys\n",
        "        #시간이 많이 걸립니다.#base line 코드입니다.\n",
        "        for text in tqdm.tqdm(train[temp_col]):#174,304건\n",
        "            try:\n",
        "                preprocessed = preprocessing(text, mecab)\n",
        "                clean_train_text.append(preprocessed)\n",
        "            except:\n",
        "                clean_train_text.append([])\n",
        "\n",
        "        for text in tqdm.tqdm(test[temp_col]):\n",
        "            if type(text) == str:\n",
        "                preprocessed = preprocessing(text, mecab)\n",
        "                clean_test_text.append(preprocessed)\n",
        "            else:\n",
        "                clean_test_text.append([])\n",
        "\n",
        "        del mecab\n",
        "        del stop_words_en\n",
        "        del test\n",
        "        del train\n",
        "\n",
        "        np.save(open(DATA_IN_PATH+'clean_train_text.npy', 'wb'), np.array(clean_train_text, dtype=object))\n",
        "        np.save(open(DATA_IN_PATH+'clean_test_text.npy', 'wb'), np.array(clean_test_text, dtype=object))\n",
        "\n",
        "    tokenizer=Tokenizer()#단어제한 없이 전부 사용 >> Unable to allocate 849. GiB for an array with shape (174304, 653541) and data type float64\n",
        "    tokenizer.fit_on_texts(clean_train_text) # fit_on_texts()안에 코퍼스를 입력으로 하면 빈도수를 기준으로 단어 집합을 생성.\n",
        "\n",
        "    word_vocab=tokenizer.word_index\n",
        "    data_configs={}\n",
        "    data_configs['vocab']=word_vocab\n",
        "    data_configs['vocab_size'] = len(word_vocab)+1\n",
        "    json.dump(data_configs, open(DATA_IN_PATH+'data_configs.json', 'w', encoding='utf8'), ensure_ascii=False)\n",
        "\n",
        "\n",
        "    train_sequences=tokenizer.texts_to_sequences(clean_train_text)\n",
        "    del clean_train_text\n",
        "    test_sequences=tokenizer.texts_to_sequences(clean_test_text)\n",
        "    del clean_test_text\n",
        "\n",
        "\n",
        "    inputs_on_train=pad_sequences(train_sequences, padding='post')\n",
        "    del train_sequences\n",
        "    test_inputs=pad_sequences(test_sequences, padding='post', maxlen=inputs_on_train.shape[-1])\n",
        "    del test_sequences\n",
        "\n",
        "\n",
        "    TRAIN_INPUT_DATA = 'train_input.npy'\n",
        "    TRAIN_LABEL_INPUT_DATA = 'label_input.npy'\n",
        "    TEST_INPUT_DATA = 'test_input.npy'\n",
        "    #baseline 코드입니다.\n",
        "    import os\n",
        "    if not os.path.exists(DATA_IN_PATH):\n",
        "        os.makedirs(DATA_IN_PATH)\n",
        "    np.save(open(DATA_IN_PATH+TRAIN_INPUT_DATA, 'wb'), inputs_on_train)#학습데이터\n",
        "    labels = pd.get_dummies(labels).to_numpy()\n",
        "    np.save(open(DATA_IN_PATH+TRAIN_LABEL_INPUT_DATA, 'wb'), labels)#학습데이터\n",
        "    np.save(open(DATA_IN_PATH+TEST_INPUT_DATA, 'wb'), test_inputs)#제출데이터\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kawwRHRpTMNr"
      },
      "source": [
        "maxlen = inputs_on_train.shape[1]\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "train_validation_data_arr, test_data, train_validation_labels, test_labels = train_test_split(inputs_on_train, labels, random_state=42)\n",
        "# 학습데이터에서 resampling을 하기전에 검증데이터를 분할합니다.\n",
        "train, validation_data_arr, train_labels, validation_labels_arr = train_test_split(train_validation_data_arr, train_validation_labels, random_state=42)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sSI3EfTe91TE"
      },
      "source": [
        "# resampling  기법 사용할때\n",
        "#학습데이터 resampling\n",
        "from sklearn.utils import resample\n",
        "train_data_sampled = None\n",
        "train_labels_sampled = None\n",
        "\n",
        "unique_lbls = np.argmax(np.unique(train_labels, axis=0), axis=1)\n",
        "\n",
        "from scipy.sparse import csr_matrix, vstack; \n",
        "for lbl in unique_lbls:\n",
        "    # indexes = np.argmax(train_labels, axis=1) ==lbl\n",
        "    # train_inputs_resampled, labels_resampled = train[indexes], train_labels[indexes]\n",
        "    # resampling\n",
        "    train_inputs_resampled, labels_resampled = resample(train[train_labels == lbl], train_labels[train_labels==lbl], replace=True, n_samples=5000, random_state=1234)\n",
        "    if isinstance(train_data_sampled, csr_matrix):\n",
        "        train_data_sampled = vstack([train_data_sampled, csr_matrix(train_inputs_resampled)])\n",
        "    else:\n",
        "        train_data_sampled = csr_matrix(train_inputs_resampled)\n",
        "\n",
        "    if isinstance(train_labels_sampled, csr_matrix):\n",
        "        train_labels_sampled = csr_matrix(vstack([train_labels_sampled, csr_matrix(labels_resampled)]))\n",
        "    else:\n",
        "        train_labels_sampled = csr_matrix(labels_resampled)\n",
        "\n",
        "del train\n",
        "del train_labels\n",
        "\n",
        "sample_len = train_data_sampled.shape[0]\n",
        "index = np.arange(sample_len)# label별로 순서대로 쌓여있어서 학습시 과적합 문제가 발생합니다.\n",
        "np.random.shuffle(index)# 각 label 별 데이터를 섞습니다\n",
        "\n",
        "train_inputs = train_data_sampled[index, :]\n",
        "del train_data_sampled\n",
        "labels = train_labels_sampled[index, :]\n",
        "del train_labels_sampled\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ucEM5JyDd8bD"
      },
      "source": [
        "vocab_size =data_configs['vocab_size']\n",
        "embedding_dim = 300"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xsXVE0cO3B0V",
        "outputId": "455620f9-ac4b-4e1c-ed63-144e86a65938"
      },
      "source": [
        "model_input = Input(shape=(train_inputs.shape[-1]))\n",
        "x1 = Embedding(vocab_size, embedding_dim)(model_input)\n",
        "x2 = GlobalAveragePooling1D()(x1)\n",
        "x3 = Dense(64, kernel_initializer='glorot_normal', kernel_regularizer=tf.keras.regularizers.l2(3))(x2)\n",
        "x4 = layers.Dropout(0.5)(x3)\n",
        "outputs = Dense(validation_labels_arr.shape[-1], activation='softmax')(x4)\n",
        "model = Model(inputs=model_input, outputs=outputs)\n",
        "print(model.summary())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 2574)]            0         \n",
            "_________________________________________________________________\n",
            "embedding (Embedding)        (None, 2574, 300)         215880600 \n",
            "_________________________________________________________________\n",
            "global_average_pooling1d (Gl (None, 300)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 64)                19264     \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 46)                2990      \n",
            "=================================================================\n",
            "Total params: 215,902,854\n",
            "Trainable params: 215,902,854\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rurYbpvqf2fL"
      },
      "source": [
        "learning_rate = 0.001\n",
        "lr_decay = tf.keras.optimizers.schedules.ExponentialDecay(learning_rate, \n",
        "                                                        labels.shape[0]/10, \n",
        "                                                        decay_rate=0.5, \n",
        "                                                        staircase=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HJi-nP74eg9Q"
      },
      "source": [
        "loss = tf.keras.losses.CategoricalCrossentropy(from_logits=False)\n",
        "f1 = tfa.metrics.F1Score(num_classes=validation_labels_arr.shape[-1], average='macro', threshold=None)\n",
        "metrics = [f1, tf.keras.metrics.CategoricalAccuracy()]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CBq5TrCAvk_p"
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam(learning_rate=lr_decay)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cuOB4hWlvPnp"
      },
      "source": [
        "model.compile(loss=loss, optimizer=optimizer, metrics=metrics)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sStpvqCjemBg"
      },
      "source": [
        "from sklearn.utils import class_weight\n",
        "\n",
        "# class_weights = class_weight.compute_class_weight('balanced'\n",
        "#                                                 , np.argmax(np.unique(train_labels, axis=0), axis=1)\n",
        "#                                                 , np.argmax(train_labels, axis=1).reshape(-1))\n",
        "# resampling 기법 사용할때\n",
        "class_weights = class_weight.compute_class_weight('balanced'\n",
        "                                                , np.argmax(np.unique(labels.todense(), axis=0), axis=1)\n",
        "                                                , np.argmax(labels.toarray(), axis=1).reshape(-1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xUMXM9bp-PWa"
      },
      "source": [
        "\n",
        "# resampling 기법 사용할때\n",
        "class CIFAR10Sequence(Sequence):\n",
        "\n",
        "    def __init__(self, x_set, y_set, batch_size):\n",
        "        self.x, self.y = x_set, y_set\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "    def __len__(self):\n",
        "        # return math.ceil(len(self.x) / self.batch_size)\n",
        "        return math.ceil(self.x.shape[0] / self.batch_size)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        indexes = np.random.choice(self.y.shape[0], self.batch_size, replace=True)\n",
        "        # batch_x = self.x[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
        "        # batch_y = self.y[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
        "        batch_x = self.x[indexes]\n",
        "        batch_y = self.y[indexes]\n",
        "\n",
        "        return (batch_x.todense(), batch_y.todense())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7hPLuiUzgO0N"
      },
      "source": [
        "MODEL_SAVE_FOLDER_PATH = '/content/drive/My Drive/mulcamnlp2021/models'\n",
        "model_file_path = f'{MODEL_SAVE_FOLDER_PATH}/DACON-{{epoch:d}}-{{val_loss:.5f}}-{{val_f1_score:.5f}}.hdf5'\n",
        "cb_model_check_point = ModelCheckpoint(filepath=model_file_path, monitor='val_f1_score', verbose=1, save_best_only=True)\n",
        "cb_early_stopping = EarlyStopping(monitor='val_loss', patience=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ViEVBdIYfjx-",
        "outputId": "3587f77e-226c-47f7-c535-f79db7a55cb5"
      },
      "source": [
        "# history = model.fit(train, train_labels\n",
        "#                     , epochs=100, batch_size=50\n",
        "#                     , validation_data=(validation_data_arr, validation_labels_arr)\n",
        "#                     , class_weight=dict(zip(np.argmax(np.unique(train_labels, axis=0), axis=1), class_weights))\n",
        "#                     ,callbacks=[cb_model_check_point, cb_early_stopping]\n",
        "#                     )\n",
        "\n",
        "# resampling 기법 사용할때\n",
        "# fit model\n",
        "num_epochs = 100\n",
        "history = model.fit(CIFAR10Sequence(x_set=train_inputs, y_set=labels, batch_size=258)\n",
        "                    , epochs=num_epochs\n",
        "                    , validation_data=(validation_data_arr, validation_labels_arr)\n",
        "                    , class_weight=dict(zip(np.argmax(np.unique(labels.todense(), axis=0), axis=1), class_weights))\n",
        "                    , callbacks=[cb_model_check_point, cb_early_stopping]\n",
        "                    )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/array_ops.py:5049: calling gather (from tensorflow.python.ops.array_ops) with validate_indices is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "The `validate_indices` argument has no effect. Indices are always validated on CPU and never validated on GPU.\n",
            "Epoch 1/50\n",
            "381/381 [==============================] - ETA: 0s - loss: 38.6466 - f1_score: 0.0051 - categorical_accuracy: 0.0146"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/metrics.py:257: UserWarning: Metric F1Score implements a `reset_states()` method; rename it to `reset_state()` (without the final \"s\"). The name `reset_states()` has been deprecated to improve API consistency.\n",
            "  'consistency.' % (self.__class__.__name__,))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r381/381 [==============================] - 474s 1s/step - loss: 38.6466 - f1_score: 0.0051 - categorical_accuracy: 0.0146 - val_loss: 3.8075 - val_f1_score: 1.9600e-04 - val_categorical_accuracy: 0.0045\n",
            "\n",
            "Epoch 00001: val_f1_score improved from inf to 0.00020, saving model to /content/drive/My Drive/mulcamnlp2021/models/DACON-1-3.80748-0.00020.hdf5\n",
            "Epoch 2/50\n",
            "381/381 [==============================] - ETA: 0s - loss: 3.8515 - f1_score: 0.0020 - categorical_accuracy: 0.0044"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/metrics.py:257: UserWarning: Metric F1Score implements a `reset_states()` method; rename it to `reset_state()` (without the final \"s\"). The name `reset_states()` has been deprecated to improve API consistency.\n",
            "  'consistency.' % (self.__class__.__name__,))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r381/381 [==============================] - 487s 1s/step - loss: 3.8515 - f1_score: 0.0020 - categorical_accuracy: 0.0044 - val_loss: 3.8033 - val_f1_score: 3.3233e-05 - val_categorical_accuracy: 7.6495e-04\n",
            "\n",
            "Epoch 00002: val_f1_score improved from 0.00020 to 0.00003, saving model to /content/drive/My Drive/mulcamnlp2021/models/DACON-2-3.80333-0.00003.hdf5\n",
            "Epoch 3/50\n",
            "381/381 [==============================] - ETA: 0s - loss: 3.8123 - f1_score: 6.9006e-04 - categorical_accuracy: 0.0026"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/metrics.py:257: UserWarning: Metric F1Score implements a `reset_states()` method; rename it to `reset_state()` (without the final \"s\"). The name `reset_states()` has been deprecated to improve API consistency.\n",
            "  'consistency.' % (self.__class__.__name__,))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r381/381 [==============================] - 489s 1s/step - loss: 3.8123 - f1_score: 6.9006e-04 - categorical_accuracy: 0.0026 - val_loss: 3.8158 - val_f1_score: 1.3299e-05 - val_categorical_accuracy: 3.0598e-04\n",
            "\n",
            "Epoch 00003: val_f1_score improved from 0.00003 to 0.00001, saving model to /content/drive/My Drive/mulcamnlp2021/models/DACON-3-3.81576-0.00001.hdf5\n",
            "Epoch 4/50\n",
            "381/381 [==============================] - ETA: 0s - loss: 3.8007 - f1_score: 9.6169e-04 - categorical_accuracy: 0.0015"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/metrics.py:257: UserWarning: Metric F1Score implements a `reset_states()` method; rename it to `reset_state()` (without the final \"s\"). The name `reset_states()` has been deprecated to improve API consistency.\n",
            "  'consistency.' % (self.__class__.__name__,))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r381/381 [==============================] - 489s 1s/step - loss: 3.8007 - f1_score: 9.6169e-04 - categorical_accuracy: 0.0015 - val_loss: 3.8254 - val_f1_score: 5.4476e-05 - val_categorical_accuracy: 0.0013\n",
            "\n",
            "Epoch 00004: val_f1_score did not improve from 0.00001\n",
            "Epoch 5/50\n",
            "381/381 [==============================] - ETA: 0s - loss: 3.8027 - f1_score: 0.0019 - categorical_accuracy: 0.0019"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/metrics.py:257: UserWarning: Metric F1Score implements a `reset_states()` method; rename it to `reset_state()` (without the final \"s\"). The name `reset_states()` has been deprecated to improve API consistency.\n",
            "  'consistency.' % (self.__class__.__name__,))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r381/381 [==============================] - 484s 1s/step - loss: 3.8027 - f1_score: 0.0019 - categorical_accuracy: 0.0019 - val_loss: 3.8183 - val_f1_score: 3.4561e-05 - val_categorical_accuracy: 7.9554e-04\n",
            "\n",
            "Epoch 00005: val_f1_score did not improve from 0.00001\n",
            "Epoch 6/50\n",
            "381/381 [==============================] - ETA: 0s - loss: 3.8219 - f1_score: 0.0021 - categorical_accuracy: 0.0040"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/metrics.py:257: UserWarning: Metric F1Score implements a `reset_states()` method; rename it to `reset_state()` (without the final \"s\"). The name `reset_states()` has been deprecated to improve API consistency.\n",
            "  'consistency.' % (self.__class__.__name__,))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r381/381 [==============================] - 484s 1s/step - loss: 3.8219 - f1_score: 0.0021 - categorical_accuracy: 0.0040 - val_loss: 3.8322 - val_f1_score: 4.1202e-05 - val_categorical_accuracy: 9.4853e-04\n",
            "\n",
            "Epoch 00006: val_f1_score did not improve from 0.00001\n",
            "Epoch 7/50\n",
            "381/381 [==============================] - ETA: 0s - loss: 3.7847 - f1_score: 0.0031 - categorical_accuracy: 0.0043"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/metrics.py:257: UserWarning: Metric F1Score implements a `reset_states()` method; rename it to `reset_state()` (without the final \"s\"). The name `reset_states()` has been deprecated to improve API consistency.\n",
            "  'consistency.' % (self.__class__.__name__,))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r381/381 [==============================] - 483s 1s/step - loss: 3.7847 - f1_score: 0.0031 - categorical_accuracy: 0.0043 - val_loss: 3.8113 - val_f1_score: 6.5089e-05 - val_categorical_accuracy: 0.0015\n",
            "\n",
            "Epoch 00007: val_f1_score did not improve from 0.00001\n",
            "Epoch 8/50\n",
            "381/381 [==============================] - ETA: 0s - loss: 3.8850 - f1_score: 0.0026 - categorical_accuracy: 0.0033"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/metrics.py:257: UserWarning: Metric F1Score implements a `reset_states()` method; rename it to `reset_state()` (without the final \"s\"). The name `reset_states()` has been deprecated to improve API consistency.\n",
            "  'consistency.' % (self.__class__.__name__,))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r381/381 [==============================] - 483s 1s/step - loss: 3.8850 - f1_score: 0.0026 - categorical_accuracy: 0.0033 - val_loss: 3.8515 - val_f1_score: 1.5958e-05 - val_categorical_accuracy: 3.6717e-04\n",
            "\n",
            "Epoch 00008: val_f1_score did not improve from 0.00001\n",
            "Epoch 9/50\n",
            "381/381 [==============================] - ETA: 0s - loss: 3.8330 - f1_score: 8.1820e-04 - categorical_accuracy: 0.0013"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/metrics.py:257: UserWarning: Metric F1Score implements a `reset_states()` method; rename it to `reset_state()` (without the final \"s\"). The name `reset_states()` has been deprecated to improve API consistency.\n",
            "  'consistency.' % (self.__class__.__name__,))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r381/381 [==============================] - 482s 1s/step - loss: 3.8330 - f1_score: 8.1820e-04 - categorical_accuracy: 0.0013 - val_loss: 3.8256 - val_f1_score: 6.5089e-05 - val_categorical_accuracy: 0.0015\n",
            "\n",
            "Epoch 00009: val_f1_score did not improve from 0.00001\n",
            "Epoch 10/50\n",
            "381/381 [==============================] - ETA: 0s - loss: 3.8789 - f1_score: 0.0011 - categorical_accuracy: 0.0015"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/metrics.py:257: UserWarning: Metric F1Score implements a `reset_states()` method; rename it to `reset_state()` (without the final \"s\"). The name `reset_states()` has been deprecated to improve API consistency.\n",
            "  'consistency.' % (self.__class__.__name__,))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r381/381 [==============================] - 483s 1s/step - loss: 3.8789 - f1_score: 0.0011 - categorical_accuracy: 0.0015 - val_loss: 3.8389 - val_f1_score: 1.0640e-05 - val_categorical_accuracy: 2.4478e-04\n",
            "\n",
            "Epoch 00010: val_f1_score improved from 0.00001 to 0.00001, saving model to /content/drive/My Drive/mulcamnlp2021/models/DACON-10-3.83895-0.00001.hdf5\n",
            "Epoch 11/50\n",
            "381/381 [==============================] - ETA: 0s - loss: 3.8434 - f1_score: 3.4016e-04 - categorical_accuracy: 8.8506e-04"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/metrics.py:257: UserWarning: Metric F1Score implements a `reset_states()` method; rename it to `reset_state()` (without the final \"s\"). The name `reset_states()` has been deprecated to improve API consistency.\n",
            "  'consistency.' % (self.__class__.__name__,))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r381/381 [==============================] - 484s 1s/step - loss: 3.8434 - f1_score: 3.4016e-04 - categorical_accuracy: 8.8506e-04 - val_loss: 3.8328 - val_f1_score: 1.9946e-05 - val_categorical_accuracy: 4.5897e-04\n",
            "\n",
            "Epoch 00011: val_f1_score did not improve from 0.00001\n",
            "Epoch 12/50\n",
            "381/381 [==============================] - ETA: 0s - loss: 3.7952 - f1_score: 9.8681e-04 - categorical_accuracy: 0.0011"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/metrics.py:257: UserWarning: Metric F1Score implements a `reset_states()` method; rename it to `reset_state()` (without the final \"s\"). The name `reset_states()` has been deprecated to improve API consistency.\n",
            "  'consistency.' % (self.__class__.__name__,))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r381/381 [==============================] - 482s 1s/step - loss: 3.7952 - f1_score: 9.8681e-04 - categorical_accuracy: 0.0011 - val_loss: 3.8284 - val_f1_score: 9.3104e-06 - val_categorical_accuracy: 2.1419e-04\n",
            "\n",
            "Epoch 00012: val_f1_score improved from 0.00001 to 0.00001, saving model to /content/drive/My Drive/mulcamnlp2021/models/DACON-12-3.82843-0.00001.hdf5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bIkZlhc7fjMs"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}